# ==========================================================
# DDPM Unlearning (SISS + EMA + AMP + Cosine LR)
# ==========================================================

project:
  name: "ddpm_mnist_unlearning"
  output_dir: "experiments/unlearned_ddpm"
  seed: 42

# ------------------------
# DATA
# ------------------------
data:
  retain_path: "data/processed/original_mnist.pt"
  forget_path: "data/processed/trousers_subset.pt"

  image_size: 64
  num_classes: 10

  batch_size: 128
  num_workers: 2
  shuffle: true

  normalize_mean: 0.5
  normalize_std: 0.5

# ------------------------
# MODEL
# ------------------------
model:
  unet:
    sample_size: 64
    in_channels: 1
    out_channels: 1
    layers_per_block: 2
    block_out_channels: [128, 256, 512]
    down_block_types: ["DownBlock2D", "AttnDownBlock2D", "DownBlock2D"]
    up_block_types: ["UpBlock2D", "AttnUpBlock2D", "UpBlock2D"]
    cross_attention_dim: 256

  class_embedding_dim: 256

  pretrained_unet_path: "experiments/ddpm/unet_final_ema.pt"
  pretrained_embedder_path: "experiments/ddpm/class_embedder.pt"

# ------------------------
# TRAINING / UNLEARNING
# ------------------------
training:
  epochs: 100

  optimizer:
    lr: 1e-5
    weight_decay: 0.0
    beta1: 0.9
    beta2: 0.999

  scheduler:
    type: "cosine"
    warmup_steps: 500

  ema:
    decay: 0.9999

  amp:
    enabled: true

# ------------------------
# DIFFUSION SCHEDULER
# ------------------------
ddpm:
  num_train_timesteps: 1000
  beta_schedule: "scaled_linear"

# ------------------------
# SISS HYPERPARAMETERS
# ------------------------
siss:
  lambda_init: 0.5
  grad_clip_ratio: 0.1
  eps: 1e-12

# ------------------------
# LOGGING & SAVING
# ------------------------
logging:
  save_plots: true
  plot_path: "experiments/unlearned_ddpm/siss_loss_curves_ema_amp.png"
  tqdm_mininterval: 0.1

saving:
  unet_output: "experiments/unlearned_ddpm/unet_unlearned_ema.pt"
  embedder_output: "experiments/unlearned_ddpm/class_embedder_unlearned_ema.pt"
  make_output_dir: true
